---
title: "GRIEG questionnaire data analysis"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(psych)
library(EFA.dimensions)
```

## Load data

We start by loading the input data. We have data from three surveys (`survey176496`, `survey381735`, and `survey977613`), each with (almost) identical structure (number of columns differ, names of some of the columns differ).

```{r}
input1 = read.table ("./input/176496/results-survey176496.csv", header = T, sep = ",")
input2 = read.table ("./input/381735/results-survey381735.csv", header = T, sep = ",")
input3 = read.table ("./input/977613/results-survey977613.csv", header = T, sep = ",")
```

`survey176496` and `survey381735` contain data from the general population (recruited by professional recruitment company), and `survey977613` contains data from climate activists (recruited via social media). 

## Filter data

Moreover, we want to filter the data based on previously performed quality check analysis. In this analysis we used the following criteria:

* consistent reporting of demographic data (sex, age)
* correct responses to 2 out of 3 control questions

As this is currently implemented in other script (written in MATLAB), we will import additional `.csv` files that contain lists of valid tokens for each survey and we will use this information to filter the data.

**NOTE:** This step is applicable only to `survey176496` and `survey381735`. Therefore, we leave `survey977613` intact. 

```{r}
valid1 = read.table ("./input/176496/valid.csv", header = T, sep = "\t")
valid2 = read.table ("./input/381735/valid.csv", header = T, sep = "\t")

dataset1 = filter(input1,token %in% valid1$token)
dataset2 = filter(input2,token %in% valid2$token)
dataset3 = input3
```

## Combine data

Next, we need to combine the datasets.

We are only interested in keeping columns that contain relevant data (questionnaire items). These data is stored in columns `14:184` in the case of `dataset1` and `dataset2`, but in columns `13:183` in the case of `dataset3`.

``` {r}
colnames(dataset1[,14:184])
```

Therefore, we combine the datasets keeping only relevant columns. Also, let's convert the final dataset to `dataframe` for convenience.

```{r}
dataset = rbind(dataset1[,14:184], dataset2[,14:184], dataset3[,13:183])
dataset = data.frame(dataset)
```

## Convert data

Initially, the imported data is in `char` format. We need to use a format more suitable for the analysis (either `factor` or `int`):

``` {r}
# recode to `factor` (categorical):
# dataset = dataset %>%
#   mutate_at(vars(1:171),
#             ~factor(recode(.,
#                            "Zdecydowanie nie" = 1,
#                            "Raczej nie " = 2,
#                            "Ani tak, ani nie" = 3,
#                            "Raczej tak " = 4,
#                            "Zdecydowanie tak" = 5)), ordered = TRUE)

# recode to `int` (numeric):
dataset = dataset %>%
  mutate_at(vars(1:171),
            list(~recode(.,
                           "Zdecydowanie nie" = 1,
                           "Raczej nie " = 2,
                           "Ani tak, ani nie" = 3,
                           "Raczej tak " = 4,
                           "Zdecydowanie tak" = 5)))
```

## Data cleaning

To perform additional cleaning of the data we look at the standard deviations (SDs) of participants' responses. Since items were presented in blocks, it should be most informative to investigate SDs within each block.

Hence, we first identify items belonging to each block. Here, we discard control questions (`CHECK`), since even participants who contributed poor quality data (pol. _klikacze_) could have made the effort to respond correctly to control questions.

``` {r}
BLOCK1 = select(dataset, contains('BLOCK1.') & !contains("CHECK"))
BLOCK2 = select(dataset, contains('BLOCK2.') & !contains("CHECK"))
BLOCK3 = select(dataset, contains('BLOCK3.') & !contains("CHECK"))
BLOCK4 = select(dataset, contains('BLOCK4.') & !contains("CHECK"))
BLOCK5 = select(dataset, contains('BLOCK5.') & !contains("CHECK"))
BLOCK6 = select(dataset, contains('BLOCK6.') & !contains("CHECK"))
BLOCK7 = select(dataset, contains('BLOCK7.') & !contains("CHECK"))
BLOCK8 = select(dataset, contains('BLOCK8.') & !contains("CHECK"))
BLOCK9 = select(dataset, contains('BLOCK9.') & !contains("CHECK"))
BLOCK10 = select(dataset, contains('BLOCK10.') & !contains("CHECK"))
BLOCK11 = select(dataset, contains('BLOCK11.') & !contains("CHECK"))
BLOCK12 = select(dataset, contains('BLOCK12.') & !contains("CHECK"))
BLOCK13 = select(dataset, contains('BLOCK13.') & !contains("CHECK"))
BLOCK14 = select(dataset, contains('BLOCK14.') & !contains("CHECK"))
```

Next, for each block separately we identify "good" participants, that is participants with nonzero variance. This results with 14 lists of indicies that represent good participants in each block.

``` {r}
BLOCKS = list(BLOCK1, BLOCK2, BLOCK3, BLOCK4, BLOCK5, BLOCK6, BLOCK7, BLOCK8, BLOCK9, BLOCK10, BLOCK11, BLOCK12, BLOCK13, BLOCK14)

good = NULL
for (i in 1:length(BLOCKS)) {
  SD = apply(BLOCKS[[i]], 1, sd) # compute SD for each participant
  good[i] = list(which(SD != 0)) # find participants with nonzero variance
  #print(length(good[[i]]))
  cat(sprintf("BLOCK%s: %s participants with nonzero variance \n", i, length(good[[i]])))
}
```

As you can see above, the list of good participants will differ from one block to the other. To identify participants with nonzero variance across blocks, we need to intersect those 14 lists.

```{r}
allgood = Reduce(intersect, good)
```

We can now clean the dataset:

```{r}
dataset = dataset[allgood,]
```

## Reliability analysis

We want to identify items that best represent a given emotion construct (e.g. anger). Specifically, for each item we want to assess how well it correlates with all the other items representing the same emotion construct. 

Thus, we first need to split the data according to emotions.

``` {r}
ANG = select(dataset,contains('.ANG')) # anger
APP = select(dataset,contains('.APP')) # apprehension
EMP = select(dataset,contains('.EMP')) # empowerment
GUI = select(dataset,contains('.GUI')) # guilt
HOPF = select(dataset,contains('.HOPF')) # hopefulness
HOPL = select(dataset,contains('.HOPL')) # hopelessness
IRR = select(dataset,contains('.IRR')) # irritation
ISO = select(dataset,contains('.ISO')) # isolation
POWL = select(dataset,contains('.POWL')) # powerlessness
SOR = select(dataset,contains('.SOR')) # sorrow

DIS = select(dataset,contains('.DIS')) # discontent
IND = select(dataset,contains('.IND')) # indifference
```

Let's start by inspecting the anger items:

``` {r}
str(ANG)
```

## Exploratory factor analysis (EFA)

First, we need to decide on the number of factors to extract. We use `RAWPAR`, that is parallel analysis of eigenvalues ([Horn, 1965](https://link.springer.com/article/10.1007/BF02289447)).

The parallel analysis procedure for deciding on the number of components or factors involves extracting eigenvalues from random data sets that parallel the actual data set with regard to the number of cases and variables. For example, if the original data set consists of 620 observations for each of 171 variables, then a series of random data matrices of this size (620 by 171) would be generated, and eigenvalues would be computed for the correlation matrices for the original, real data and for each of the random data sets. The eigenvalues derived from the actual data are then compared to the eigenvalues derived from the random data.  In Horn's original description of this procedure, the mean eigenvalues from the random data served as the comparison baseline, whereas the more common current practice is to use the eigenvalues that correspond to the desired percentile (typically the 95th) of the distribution of random data eigenvalues. Factors or components are retained as long as the ith eigenvalue from the actual data is greater than the ith eigenvalue from the random data.

**NOTE:** We use `polychoric` correlations!

``` {r}
#RAWPAR(dataset, randtype = "permuted", Ndatasets=1000, factormodel="PCA", percentile=95, corkind="polychoric", corkindRAND="polychoric", verbose=T)
```

We can now proceed to the exploratory factor analysis (EFA). We assume the number of factors to extract based on the result of the `RAWPAR` analysis.

**NOTE:** We use `polychoric` correlations!

``` {r, message = FALSE, results = 'hide'}
PA = PA_FA(dataset, corkind="polychoric", Nfactors=6, iterpaf=250, rotate="none", verbose=T)
```

Next, we perform factor rotation in a separate step.

``` {r, message = FALSE, results = 'hide'}
loadings = PA$loadingsNOROT
rotated = PROMAX(loadings, ppower=4, verbose=T)
rotated = rotated$pattern
```

And finally, we clean the output with a chosen threshold. This step is only done to facilitate the decision process.

``` {r}
(rotated = ifelse(abs(rotated)<0.4, '', rotated))
```

``` {r}

```

## Note

```{r, echo=FALSE}
# use this block, to print output only (no r syntax)
```

Note that the `echo = FALSE` parameter was added to some of the R code chunks to prevent them from printing.
