---
title: "GRIEG questionnaire data analysis"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.kable.NA = '')

library(knitr)
library(dplyr)
library(psych)
library(doBy)
library(EFA.dimensions)
```

## Load items

Before we start, let's load the content of the questionnaire items. The `items.tsv` file contains unique item codes, as well as item text in Polish and English.
 
```{r}
items =  read.table ("./input/items.tsv", header = T, sep = "\t", encoding="UTF-8")
```

## Load data

We start by loading the input data. We have data from three surveys (`survey176496`, `survey381735`, and `survey977613`), each with (almost) identical structure (number of columns differ, names of some of the columns differ). Input data files contain demographical data of participants and answers to questionnaire items.

```{r}
input1 = read.table ("./input/176496/results-survey176496.csv", header = T, sep = ",", encoding="UTF-8")
input2 = read.table ("./input/381735/results-survey381735.csv", header = T, sep = ",", encoding="UTF-8")
input3 = read.table ("./input/977613/results-survey977613.csv", header = T, sep = ",", encoding="UTF-8")
```

`survey176496` and `survey381735` contain data from the general population (recruited by professional recruitment company), and `survey977613` contains data from climate activists (recruited via social media). 

## Filter data

Moreover, we want to filter the data based on previously performed quality check analysis. In this analysis we used the following criteria:

* consistent reporting of demographic data (sex, age)
* correct responses to 2 out of 3 control questions

As this is currently implemented in other script (written in MATLAB), we will import additional `.csv` files that contain lists of valid tokens for each survey and we will use this information to filter the data.

**NOTE:** This step is applicable only to `survey176496` and `survey381735`. Therefore, we leave `survey977613` intact. 

```{r}
valid1 = read.table ("./input/176496/valid.csv", header = T, sep = "\t", encoding="UTF-8")
valid2 = read.table ("./input/381735/valid.csv", header = T, sep = "\t", encoding="UTF-8")

dataset1 = filter(input1,token %in% valid1$token)
dataset2 = filter(input2,token %in% valid2$token)
dataset3 = input3
```

## Combine data

Next, we need to combine the datasets.

We are only interested in keeping columns that contain relevant data (questionnaire items). These data is stored in columns `14:184` in the case of `dataset1` and `dataset2`, but in columns `13:183` in the case of `dataset3`.

``` {r}
colnames(dataset1[,14:184])
```

Therefore, we combine the datasets keeping only relevant columns. Also, let's convert the final dataset to `dataframe` for convenience.

```{r}
dataset = rbind(dataset1[,14:184], dataset2[,14:184], dataset3[,13:183])
dataset = data.frame(dataset)
```

### Getting rid of extra spaces 
```{r}
library(stringr)

dataset = as.data.frame(apply(dataset,2,str_trim))
```
## Convert data

Initially, the imported data is in `char` format. We need to use a format more suitable for the analysis (either `factor` or `int`):

``` {r}
# recode to `factor` (categorical):
# dataset = dataset %>%
#   mutate_at(vars(1:171),
#             ~factor(recode(.,
#                            "Zdecydowanie nie" = 1,
#                            "Raczej nie " = 2,
#                            "Ani tak, ani nie" = 3,
#                            "Raczej tak " = 4,
#                            "Zdecydowanie tak" = 5)), ordered = TRUE)

# recode to `int` (numeric



dataset = dataset %>%
  mutate_at(vars(1:171),
            list(~recode(.,
                           "Zdecydowanie nie" = 1,
                           "Raczej nie" = 2,
                           "Ani tak, ani nie" = 3,
                           "Raczej tak" = 4,
                           "Zdecydowanie tak" = 5)))
```

## Data cleaning

To perform additional cleaning of the data we look at the standard deviations (SDs) of participants' responses. Since items were presented in blocks, it should be most informative to investigate SDs within each block.

Hence, we first identify items belonging to each block. Here, we discard control questions (`CHECK`), since even participants who contributed poor quality data (pol. _klikacze_) could have made the effort to respond correctly to control questions.

``` {r}
BLOCK1 = select(dataset, contains('BLOCK1.') & !contains("CHECK"))
BLOCK2 = select(dataset, contains('BLOCK2.') & !contains("CHECK"))
BLOCK3 = select(dataset, contains('BLOCK3.') & !contains("CHECK"))
BLOCK4 = select(dataset, contains('BLOCK4.') & !contains("CHECK"))
BLOCK5 = select(dataset, contains('BLOCK5.') & !contains("CHECK"))
BLOCK6 = select(dataset, contains('BLOCK6.') & !contains("CHECK"))
BLOCK7 = select(dataset, contains('BLOCK7.') & !contains("CHECK"))
BLOCK8 = select(dataset, contains('BLOCK8.') & !contains("CHECK"))
BLOCK9 = select(dataset, contains('BLOCK9.') & !contains("CHECK"))
BLOCK10 = select(dataset, contains('BLOCK10.') & !contains("CHECK"))
BLOCK11 = select(dataset, contains('BLOCK11.') & !contains("CHECK"))
BLOCK12 = select(dataset, contains('BLOCK12.') & !contains("CHECK"))
BLOCK13 = select(dataset, contains('BLOCK13.') & !contains("CHECK"))
BLOCK14 = select(dataset, contains('BLOCK14.') & !contains("CHECK"))
```

Next, for each block separately we identify "good" participants, that is participants with nonzero variance of answers. This results with 14 lists of indices that represent good participants in each block.

``` {r}
BLOCKS = list(BLOCK1, BLOCK2, BLOCK3, BLOCK4, BLOCK5, BLOCK6, BLOCK7, BLOCK8, BLOCK9, BLOCK10, BLOCK11, BLOCK12, BLOCK13, BLOCK14)

good = NULL
for (i in 1:length(BLOCKS)) {
  SD = apply(BLOCKS[[i]], 1, sd) # compute SD for each participant
  good[i] = list(which(SD != 0)) # find participants with nonzero variance
  #print(length(good[[i]]))
  cat(sprintf("BLOCK%s: %s participants with nonzero variance \n", i, length(good[[i]])))
}
```

As you can see above, the list of good participants will differ from one block to the other. To identify participants with nonzero variance across blocks, we need to intersect those 14 lists.

```{r}
allgood = Reduce(intersect, good)
```

We can now clean the dataset:

```{r}
dataset = dataset[allgood,]
```

## Reliability analysis {.tabset}

We want to identify items that best represent a given emotion construct (e.g. anger). Specifically, for each item we want to assess how well it correlates with all the other items representing the same emotion construct. 

Thus, we first need to split the data according to emotions.

``` {r}
ANG = select(dataset,contains('.ANG')) # Anger
APP = select(dataset,contains('.APP')) # Apprehension
EMP = select(dataset,contains('.EMP')) # Empowerment
GUI = select(dataset,contains('.GUI')) # Guilt
HOPF = select(dataset,contains('.HOPF')) # Hopefulness
HOPL = select(dataset,contains('.HOPL')) # Hopelessness
IRR = select(dataset,contains('.IRR')) # Irritation
ISO = select(dataset,contains('.ISO')) # Isolation
POWL = select(dataset,contains('.POWL')) # Powerlessness
SOR = select(dataset,contains('.SOR')) # Sorrow

DIS = select(dataset,contains('.DIS')) # Discontent
IND = select(dataset,contains('.IND')) # Indifference
```

We assess the reliability, by calculating Cronbach's alpha for each emotion separately. Use the tabs below to switch between the results for each emotion.

``` {r, results='asis'}
EMOTIONS = list(ANG, APP, EMP, GUI, HOPF, HOPL, IRR, ISO, POWL, SOR, DIS, IND)
labels = list("Anger", "Apprehension", "Empowerment", "Guilt",
              "Hopefulness", "Hopelessness", "Irritation", "Isolation",
              "Powerlessness", "Sorrow", "Discontent", "Indifference")

for (i in 1:length(EMOTIONS)) {
  
  out = alpha(EMOTIONS[[i]]) # Cronbach's alpha
  
  cat("\n")
  cat(sprintf("### %s \n", labels[[i]]))
  cat("\n")
  
  cat(sprintf("\n Reliability analysis:"))
  print(kable(out$total, digits = 2))
  
  cat(sprintf("\n Reliability if an item is dropped:"))
  print(kable(out$alpha.drop, digits = 2))
  
  cat(sprintf("\n Item statistics:"))
  print(kable(out$item.stats, digits = 2))
  
  cat(sprintf("\n Non missing response frequency for each item:"))
  print(kable(out$response.freq, digits = 2))
  
  cat("\n")
}
```

## Exploratory factor analysis (EFA) {.tabset}

First we need a correlation matrix for further steps
``` {r}
cor_matrix = POLYCHORIC_R(dataset, method = "Fox", verbose = F)

```

We need to decide on the number of factors to extract. We use `RAWPAR`, that is parallel analysis of eigenvalues ([Horn, 1965](https://link.springer.com/article/10.1007/BF02289447)).

The parallel analysis procedure for deciding on the number of components or factors involves extracting eigenvalues from random data sets that parallel the actual data set with regard to the number of cases and variables. For example, if the original data set consists of 620 observations for each of 171 variables, then a series of random data matrices of this size (620 by 171) would be generated, and eigenvalues would be computed for the correlation matrices for the original, real data and for each of the random data sets. The eigenvalues derived from the actual data are then compared to the eigenvalues derived from the random data.  In Horn's original description of this procedure, the mean eigenvalues from the random data served as the comparison baseline, whereas the more common current practice is to use the eigenvalues that correspond to the desired percentile (typically the 95th) of the distribution of random data eigenvalues. Factors or components are retained as long as the ith eigenvalue from the actual data is greater than the ith eigenvalue from the random data.

For the final report we should use Ndatasets = 10000

**NOTE:** We use `polychoric` correlations!

``` {r}
library(EFA.dimensions)
RAWPAR(cor_matrix, randtype = "permuted", Ndatasets=1000, factormodel="PCA", percentile=95, corkindRAND="polychoric", verbose=T, Ncases = nrow(dataset))
```

Before we proceed with EFA we calculate the Bartlett test and KMO test.

``` {r}
cortest.bartlett(cor_matrix,n=200)

KMO(cor_matrix)

```

Tests confirmed that our data are suitable for EFA. We can now proceed to the exploratory factor analysis (EFA). We assume the number of factors to extract based on the result of the `RAWPAR` analysis.

**NOTE:** We use `polychoric` correlations!


``` {r, message = FALSE, results = 'hide'}
PA = PA_FA(cor_matrix, Nfactors=6, iterpaf=250, rotate="none", verbose=T)
```

Next, we perform factor rotation in a separate step.

``` {r, message = FALSE, results = 'hide'}
loadings = PA$loadingsNOROT
rotated = PROMAX(loadings, ppower=4, verbose=T)
rotated = rotated$pattern
```

And finally, we clean the output with a chosen threshold. This step is only done to facilitate the decision process. 

``` {r, echo=FALSE}
#rotated = ifelse(abs(rotated) > 0.4, rotated, NA)
#kable(rotated)
```

``` {r}
thresholded = matrix(, nrow = nrow(rotated), ncol = ncol(rotated))
colnames(thresholded) = colnames(rotated)
rownames(thresholded) = rownames(rotated)

for (i in 1:nrow(rotated)) {
  
  tmp = unname(rotated[i,])
  
  idx = which.maxn(abs(tmp), n = 2)
  max1 = idx[1]
  max2 = idx[2]
  
  criterion1 = abs(tmp[max1]) > 0.5
  criterion2 = ( abs(tmp[max1]) - abs(tmp[max2]) ) > 0.3
  
  test = rep(FALSE, length(tmp))
  test[max1] = criterion1 & criterion2
  
  thresholded[i,] = ifelse(test, tmp, NA)
}
```

In order to make decision on which items should be used in the questionnaire, we need to test how many items end up loading each factor at different arbitrarily set thresholds in the criteria above. Each factor should constitute a subscale with a similar, small number of items with highest loadings.

```{r}
subscale_lenght <-matrix(nrow=1, ncol=ncol(thresholded))
colnames(subscale_lenght)=colnames(thresholded)
rownames(subscale_lenght)= "no of items"
for (i in 1:ncol(thresholded)) {
subscale_lenght[1,i] = sum(!is.na(thresholded[,i]))
}
```

The output matrix shows the number of items in each subscale. Modify criterion1 and criterion2 to test how the number of item changes with different critieria.

```{r}
kable(subscale_lenght)
```

Use the tabs below to switch between thresholded and unthresholded factor loadings.

### Thresholded

``` {r}
kable(thresholded, digits = 2)
```

### Unthresholded

``` {r}
kable(rotated, digits = 2)
```

``` {r}

```

## Note

```{r, echo=FALSE}
# use this block, to print output only (no r syntax)
```

Note that the `echo = FALSE` parameter was added to some of the R code chunks to prevent them from printing.
